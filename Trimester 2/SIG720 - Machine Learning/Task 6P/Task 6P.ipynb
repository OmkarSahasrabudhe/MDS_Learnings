{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5336654c",
   "metadata": {},
   "source": [
    "# Task 6P"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ab6f4ca",
   "metadata": {},
   "source": [
    "The MNIST dataset is a widely used benchmark dataset for training and testing machine learning algorithms, particularly those for image recognition and classification. It consists of 60,000 training images and 10,000 testing images, each of which is a 28x28 grayscale image representing a handwritten digit from 0 to 9."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b9d491",
   "metadata": {},
   "source": [
    "#### Key Characteristics:\n",
    "\n",
    "- **Size**: 70,000 images in total (60,000 for training, 10,000 for testing)\n",
    "- **Image Size**: 28x28 pixels\n",
    "- **Format**: Grayscale images\n",
    "- **Labels**: Each image is associated with a corresponding digit label (0-9)\n",
    "- **Distribution**: The dataset is relatively balanced, with approximately the same number of images for each digit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ef8607",
   "metadata": {},
   "source": [
    "#### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "11120137",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee3c5b97",
   "metadata": {},
   "source": [
    "#### Load the MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a2127912",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set shape: (60000, 784) (60000, 10)\n",
      "Test set shape: (10000, 784) (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "# Load the MNIST dataset\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Normalize the image data to [0, 1]\n",
    "x_train = x_train.reshape(60000, 784).astype('float32') / 255\n",
    "x_test = x_test.reshape(10000, 784).astype('float32') / 255\n",
    "\n",
    "# Convert class labels to one-hot encoded vectors\n",
    "num_classes = 10\n",
    "y_train = to_categorical(y_train, num_classes)\n",
    "y_test = to_categorical(y_test, num_classes)\n",
    "\n",
    "# Print the shape of the training and test sets\n",
    "print(\"Training set shape:\", x_train.shape, y_train.shape)\n",
    "print(\"Test set shape:\", x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8a7655e",
   "metadata": {},
   "source": [
    "#### Create the MLP model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "44a12643",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-30 20:03:46.465389: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-08-30 20:03:46.694836: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:176] None of the MLIR Optimization Passes are enabled (registered 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.3620 - accuracy: 0.8992 - val_loss: 0.1956 - val_accuracy: 0.9433\n",
      "Epoch 2/10\n",
      "469/469 [==============================] - 0s 966us/step - loss: 0.1658 - accuracy: 0.9524 - val_loss: 0.1361 - val_accuracy: 0.9610\n",
      "Epoch 3/10\n",
      "469/469 [==============================] - 0s 954us/step - loss: 0.1181 - accuracy: 0.9661 - val_loss: 0.1092 - val_accuracy: 0.9693\n",
      "Epoch 4/10\n",
      "469/469 [==============================] - 0s 956us/step - loss: 0.0926 - accuracy: 0.9731 - val_loss: 0.0962 - val_accuracy: 0.9716\n",
      "Epoch 5/10\n",
      "469/469 [==============================] - 0s 962us/step - loss: 0.0750 - accuracy: 0.9783 - val_loss: 0.0892 - val_accuracy: 0.9732\n",
      "Epoch 6/10\n",
      "469/469 [==============================] - 0s 944us/step - loss: 0.0620 - accuracy: 0.9818 - val_loss: 0.0814 - val_accuracy: 0.9750\n",
      "Epoch 7/10\n",
      "469/469 [==============================] - 0s 934us/step - loss: 0.0518 - accuracy: 0.9849 - val_loss: 0.0802 - val_accuracy: 0.9751\n",
      "Epoch 8/10\n",
      "469/469 [==============================] - 0s 937us/step - loss: 0.0435 - accuracy: 0.9876 - val_loss: 0.0762 - val_accuracy: 0.9754\n",
      "Epoch 9/10\n",
      "469/469 [==============================] - 0s 935us/step - loss: 0.0369 - accuracy: 0.9899 - val_loss: 0.0787 - val_accuracy: 0.9762\n",
      "Epoch 10/10\n",
      "469/469 [==============================] - 0s 941us/step - loss: 0.0315 - accuracy: 0.9913 - val_loss: 0.0731 - val_accuracy: 0.9771\n",
      "313/313 [==============================] - 0s 489us/step - loss: 0.0731 - accuracy: 0.9771\n",
      "Test accuracy: 0.9771000146865845\n"
     ]
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    Dense(128, activation='relu', input_shape=(784,)),\n",
    "    Dense(num_classes, activation='softmax')])\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(x_train, y_train, epochs=10, batch_size=128, validation_data=(x_test, y_test))\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
    "print(\"Test accuracy:\", test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4188cd45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hidden layer: 2 - Test accuracy: 0.9776999950408936\n",
      "Hidden layer: 4 - Test accuracy: 0.9781000018119812\n",
      "Hidden layer: 6 - Test accuracy: 0.9757000207901001\n",
      "Hidden layer: 8 - Test accuracy: 0.9710999727249146\n",
      "Hidden layer: 10 - Test accuracy: 0.9747999906539917\n"
     ]
    }
   ],
   "source": [
    "hidden_layers = [2, 4, 6, 8, 10]\n",
    "for num_layers in hidden_layers:\n",
    "    model = Sequential()\n",
    "    for _ in range(num_layers):\n",
    "        model.add(Dense(100, activation='relu'))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    model.fit(x_train, y_train, epochs=10, batch_size=128, verbose=0)\n",
    "    test_loss, test_acc = model.evaluate(x_test, y_test, verbose=0)\n",
    "    print(f\"Hidden layer: {num_layers} - Test accuracy: {test_acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3420a76c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hidden layer size: 50 - Test accuracy: 0.9695000052452087\n",
      "Hidden layer size: 100 - Test accuracy: 0.9775999784469604\n",
      "Hidden layer size: 150 - Test accuracy: 0.978600025177002\n",
      "Hidden layer size: 200 - Test accuracy: 0.979200005531311\n"
     ]
    }
   ],
   "source": [
    "hidden_sizes = [50, 100, 150, 200]\n",
    "for hidden_size in hidden_sizes:\n",
    "    model = Sequential([\n",
    "        Dense(hidden_size, activation='relu', input_shape=(784,)),\n",
    "        Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    model.fit(x_train, y_train, epochs=10, batch_size=128, verbose=0)\n",
    "    test_loss, test_acc = model.evaluate(x_test, y_test, verbose=0)\n",
    "    print(f\"Hidden layer size: {hidden_size} - Test accuracy: {test_acc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7819085",
   "metadata": {},
   "source": [
    "### Key Findings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a98a639",
   "metadata": {},
   "source": [
    "- Impact of Number of Hidden Layers: Increasing the number of hidden layers generally improves accuracy up to a certain point. Beyond that, adding more layers might lead to overfitting.\n",
    "- Impact of Hidden Layer Size: Increasing the hidden layer size also improves accuracy up to a certain point. However, excessively large hidden layers can also lead to overfitting.\n",
    "- Optimal Configuration: The optimal number of hidden layers and hidden layer size depends on the specific dataset and problem. It's often found through experimentation and techniques like hyperparameter tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615324f4",
   "metadata": {},
   "source": [
    "_____"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
